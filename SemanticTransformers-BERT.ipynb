{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d8f83c",
   "metadata": {},
   "source": [
    "<h1 style=\"color: blue; font-style: italic; font-family: sans-serif; text-align: center;\">Revolutionizing Recommendations as a Personalization Strategy:<p style=\"color:brown; text-align: center;\">Semantic Transformer</p></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e999cb4",
   "metadata": {},
   "source": [
    "### [Article: Revolutionizing Recommendations as a Personalization Strategy: Semantic Transformer](https://medium.com/@shukla.shankar.ravi/revolutionizing-recommendations-as-a-personalization-strategy-semantic-transformer-05d7ba545a45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9319ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee2c6365",
   "metadata": {},
   "source": [
    "# Semantic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ec702e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tovin\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil  # Required for removing non-empty directories\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(9973)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a75d54",
   "metadata": {},
   "source": [
    "## Step 1: Initiate Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fffaf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChromaDB not found, starting fresh.\n"
     ]
    }
   ],
   "source": [
    "# Path where ChromaDB is stored\n",
    "chroma_db_path = './chroma_db'\n",
    "\n",
    "# Function to drop ChromaDB\n",
    "def reset_chroma_db(chroma_db_path):\n",
    "    \"\"\"\n",
    "    Deletes the existing ChromaDB directory to reset the vector database.\n",
    "\n",
    "    Args:\n",
    "        chroma_db_path (str): Filesystem path to the ChromaDB directory.\n",
    "\n",
    "    Behavior:\n",
    "        - If the directory exists, it is removed entirely.\n",
    "        - If it doesn't exist, informs the user that there's nothing to remove.\n",
    "\n",
    "    Edge Cases:\n",
    "        - Handles case where the directory does not exist (avoids error).\n",
    "        - Uses shutil.rmtree instead of os.rmdir to handle non-empty directories.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove ChromaDB directory if it exists to reset the database\n",
    "    if os.path.exists(chroma_db_path):\n",
    "        print(\"Dropping ChromaDB...\")\n",
    "        # shutil.rmtree is used here instead of os.rmdir because os.rmdir only works for empty directories\n",
    "        shutil.rmtree(chroma_db_path)\n",
    "        #os.rmdir(chroma_db_path)  # Remove the directory\n",
    "    else:\n",
    "        print(\"ChromaDB not found, starting fresh.\")\n",
    "\n",
    "# Call the function to reset the ChromaDB before initializing a new vector store\n",
    "reset_chroma_db(chroma_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc85b15",
   "metadata": {},
   "source": [
    "## Step 2. Load the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07218e3",
   "metadata": {},
   "source": [
    "#### Note: Data generated Using Association Rule Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3c142",
   "metadata": {},
   "source": [
    "[Refer: Revolutionizing Recommendations as a Personalization Strategy: Traditional Approaches](https://github.com/rs-shukla/Revolutionizing-Recommendations-as-a-Personalization-Strategy/blob/main/TraditionalApproaches-AssociationRuleMining(ARM).ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba2bf3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  antecedents                 consequents  support  \\\n",
      "0      frozenset({'Bananas'})      frozenset({'Avocado'})  0.16774   \n",
      "1      frozenset({'Avocado'})      frozenset({'Bananas'})  0.16774   \n",
      "2  frozenset({'Black beans'})      frozenset({'Avocado'})  0.16604   \n",
      "3      frozenset({'Avocado'})  frozenset({'Black beans'})  0.16604   \n",
      "4  frozenset({'Blueberries'})      frozenset({'Avocado'})  0.16508   \n",
      "\n",
      "   confidence      lift  \n",
      "0    0.408544  0.996352  \n",
      "1    0.409082  0.996352  \n",
      "2    0.405986  0.990112  \n",
      "3    0.404936  0.990112  \n",
      "4    0.404628  0.986801  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "fields = ['antecedents', 'consequents', 'support', 'confidence', 'lift']\n",
    "df = pd.read_csv('./data/rules_apriori_dataset.csv', usecols=fields)\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693173eb",
   "metadata": {},
   "source": [
    "## Step 3. Initialize BERT tokenizer and model for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41f2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ddabc7",
   "metadata": {},
   "source": [
    "## Step 4. Generates BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c821358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embeddings(texts):\n",
    "    \"\"\"\n",
    "    Generates BERT embeddings for a list of input texts using mean pooling.\n",
    "\n",
    "    Args:\n",
    "        texts (List[str]): A list of input text strings to encode.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A 2D NumPy array where each row is the embedding of a text.\n",
    "\n",
    "    Logic:\n",
    "        - Tokenizes the input texts with padding and truncation to handle varying lengths.\n",
    "        - Runs the tokenized inputs through the BERT model without tracking gradients (inference mode).\n",
    "        - Applies mean pooling over the token embeddings (last_hidden_state) to produce a single fixed-size vector per text.\n",
    "\n",
    "    Edge Case Handling:\n",
    "        - `padding=True`: Ensures all input texts are padded to the same length for batch processing.\n",
    "        - `truncation=True`: Prevents overly long texts from causing errors by truncating to the model's max length.\n",
    "        - `with torch.no_grad()`: Reduces memory usage and speeds up inference by disabling gradient calculation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize input texts with padding and truncation to create uniform tensor input\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Disable gradient tracking for inference to improve efficiency\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Apply mean pooling across token embeddings to get a single embedding per input text\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling to get a single vector\n",
    "    \n",
    "    # Convert PyTorch tensor to NumPy array for easier downstream usage\n",
    "    return embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55098e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process antecedents and consequents for text search\n",
    "df['antecedent_str'] = df['antecedents'].apply(lambda x: str(eval(x)))  # Convert frozenset to string\n",
    "df['consequent_str'] = df['consequents'].apply(lambda x: str(eval(x)))  # Convert frozenset to string\n",
    "df['antecedent_item_count'] = df['antecedent_str'].apply(lambda x: len(eval(x)))  # Count number of items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac1a11",
   "metadata": {},
   "source": [
    "## Step 5. Create a list of documents for the Chroma vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80598ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = []\n",
    "for idx, row in df.iterrows():\n",
    "    antecedent = row['antecedent_str']\n",
    "    consequent = row['consequent_str']\n",
    "    metadata = {\n",
    "        'support': row['support'],\n",
    "        'confidence': row['confidence'],\n",
    "        'lift': row['lift'],\n",
    "        'consequent_str': consequent,  # Store consequent as part of metadata\n",
    "        'antecedent_item_count': row['antecedent_item_count']  # Add item count to metadata\n",
    "    }\n",
    "    # Both antecedent and consequent are part of the same document, so we use the same document ID (idx)\n",
    "    document = Document(\n",
    "        page_content=antecedent,  # Antecedent will be the main text (page_content)\n",
    "        metadata=metadata,        # Metadata contains the consequent and other info\n",
    "        id=str(idx)               # Using the row index as a unique document ID\n",
    "    )\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a6b63",
   "metadata": {},
   "source": [
    "## Step 6. Use HuggingFaceEmbeddings directly in Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c159a3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78917584",
   "metadata": {},
   "source": [
    "## Step 7. Create Chroma vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f3fd153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorstore = Chroma.from_documents(documents, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c202b7e",
   "metadata": {},
   "source": [
    "## Step 8. Search for similar antecedent rules and return top resulting consequents based on confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1c34a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_antecedent(query, k=5, result_k=5, metadata_filter=None):\n",
    "    \"\"\"\n",
    "    Search for similar antecedent rules and return top resulting consequents based on confidence.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): Comma-separated list of input items to search for.\n",
    "    - k (int): Number of similar documents to retrieve from vectorstore.\n",
    "    - result_k (int): Number of top results to return after sorting by confidence.\n",
    "    - metadata_filter (dict): Optional filter to narrow down search scope.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples: (consequent item(s), confidence score)\n",
    "    \"\"\"\n",
    "    query_items = query.split(',')\n",
    "    #print(f\"Query : {query_items}\")\n",
    "    \n",
    "    # Perform similarity search\n",
    "    search_results = vectorstore.similarity_search(query, k=k, filter=metadata_filter)\n",
    "    #print(f\"search_results: {search_results} \\n\\n\")\n",
    "    \n",
    "    filtered_results = []\n",
    "\n",
    "    for doc in search_results:\n",
    "        metadata = doc.metadata\n",
    "        confidence = metadata.get(\"confidence\", 0)\n",
    "        \n",
    "        # Safely evaluate frozenset string to extract item(s)\n",
    "        consequent_raw = metadata.get(\"consequent_str\", \"\")\n",
    "        try:\n",
    "            consequent_set = eval(consequent_raw)\n",
    "            if isinstance(consequent_set, frozenset):\n",
    "                consequent = ', '.join(consequent_set)\n",
    "            else:\n",
    "                consequent = str(consequent_set)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing consequent: {e}\")\n",
    "            consequent = consequent_raw\n",
    "        \n",
    "        filtered_results.append((consequent, confidence))\n",
    "\n",
    "    # Sort by confidence and limit the number of results\n",
    "    filtered_results = sorted(filtered_results, key=lambda x: x[1], reverse=True)[:result_k]\n",
    "\n",
    "    #print(\"Top Consequents by Confidence:\")\n",
    "    #for item, conf in filtered_results:\n",
    "        #print(f\"{item}: {conf:.2f}\")\n",
    "    \n",
    "    return filtered_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd93a6",
   "metadata": {},
   "source": [
    "## Step 9. Generate Recommendations based on query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21332f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Consequents:\n",
      "\n",
      " [('Oranges', 0.4115072224113613), ('Russet Potatoes', 0.4099600039020583), ('Bananas', 0.4090820407765096), ('Green Grapes', 0.4089357135889181), ('Strawberries', 0.4084023038575093)]\n"
     ]
    }
   ],
   "source": [
    "# Example: When a user selects a single item, the recommendation system suggests related items based on confidence scores.\n",
    "# (Different metrics may be used depending on the specific use case.)\n",
    "query = 'Avocado'  # User query for antecedent\n",
    "\n",
    "#metadata_filter = {\"antecedent_item_count\": 1}  # You can filter based on category, author, or any metadata field\n",
    "metadata_filter = {\n",
    "    \"$and\": [{\n",
    "    \"confidence\": {\"$gte\": 0.4}\n",
    "    },  # Filter for confidence >= 0.4\n",
    "    {\"antecedent_item_count\": {\"$eq\": 1}\n",
    "    }# Filter for antecedent_item_count == 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Retrieve top-k similar consequents based on the antecedent query\n",
    "recommended_consequents = search_antecedent(query, k=50, result_k=5, metadata_filter=metadata_filter)\n",
    "print(f'Recommended Consequents:\\n\\n {recommended_consequents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3a3f8da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Consequents:\n",
      "\n",
      " [('Bananas', 0.4120458891013384), ('Green Grapes', 0.4115379983138624), ('Eggs', 0.4112093690248566), ('Red Grapes', 0.4097694971933596), ('Potatoes', 0.4090583173996175)]\n"
     ]
    }
   ],
   "source": [
    "# Example: When a user selects two items, the recommendation system suggests related items based on confidence scores.\n",
    "# (Different metrics may be used depending on the specific use case.)\n",
    "query = 'Avocado,Russet Potatoes'  # User query for antecedent\n",
    "#metadata_filter = {\"antecedent_item_count\": 1}  # You can filter based on category, author, or any metadata field\n",
    "metadata_filter = {\n",
    "    \"$and\": [{\n",
    "    \"confidence\": {\"$gte\": 0.4}\n",
    "    },  # Filter for confidence >= 0.4\n",
    "    {\"antecedent_item_count\": {\"$eq\": 2}\n",
    "    }# Filter for antecedent_item_count == 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Retrieve top-k similar consequents based on the antecedent query\n",
    "recommended_consequents = search_antecedent(query, k=50, result_k=5, metadata_filter=metadata_filter)\n",
    "print(f'Recommended Consequents:\\n\\n {recommended_consequents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e78735c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Consequents:\n",
      "\n",
      " [('Potatoes', 0.4147862232779097), ('Potatoes', 0.4140982606042112), ('Oranges', 0.4127646883388011), ('Bananas', 0.4122728626750074), ('Eggs', 0.4112759643916914)]\n"
     ]
    }
   ],
   "source": [
    "# Example: When a user selects three items, the recommendation system suggests related items based on confidence scores.\n",
    "# (Different metrics may be used depending on the specific use case.)\n",
    "query = 'Avocado,Russet Potatoes,Bananas'  # User query for antecedent\n",
    "#metadata_filter = {\"antecedent_item_count\": 1}  # You can filter based on category, author, or any metadata field\n",
    "metadata_filter = {\n",
    "    \"$and\": [{\n",
    "    \"confidence\": {\"$gte\": 0.4}\n",
    "    },  # Filter for confidence >= 0.4\n",
    "    {\"antecedent_item_count\": {\"$eq\": 3}\n",
    "    }# Filter for antecedent_item_count == 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Retrieve top-k similar consequents based on the antecedent query\n",
    "recommended_consequents = search_antecedent(query, k=50, result_k=5, metadata_filter=metadata_filter)\n",
    "print(f'Recommended Consequents:\\n\\n {recommended_consequents}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d27a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Consequents:\n",
      "\n",
      " [('Potatoes', 0.4233176838810641), ('Milk', 0.4213313161875945), ('Bananas', 0.4213313161875945), ('Gala Apples', 0.4183055975794251), ('Eggs', 0.4176646706586826)]\n"
     ]
    }
   ],
   "source": [
    "# Example: When a user selects four items, the recommendation system suggests related items based on confidence scores.\n",
    "# (Different metrics may be used depending on the specific use case.)\n",
    "query = 'Avocado,Russet Potatoes,Bananas,Strawberries'  # User query for antecedent\n",
    "#metadata_filter = {\"antecedent_item_count\": 1}  # You can filter based on category, author, or any metadata field\n",
    "metadata_filter = {\n",
    "    \"$and\": [{\n",
    "    \"confidence\": {\"$gte\": 0.4}\n",
    "    },  # Filter for confidence >= 0.4\n",
    "    {\"antecedent_item_count\": {\"$eq\": 4}\n",
    "    }# Filter for antecedent_item_count == 1\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Retrieve top-k similar consequents based on the antecedent query\n",
    "recommended_consequents = search_antecedent(query, k=50, metadata_filter=metadata_filter)\n",
    "print(f'Recommended Consequents:\\n\\n {recommended_consequents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b753cc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
